{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ual\\.conda\\envs\\felix\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x252200630a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import seq_generator\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "\n",
    "train_ds , val_ds = seq_generator.get_train_and_val(batch_size=32)\n",
    "cnn = tf.keras.models.load_model(\"cnn_base/model\")\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "\n",
    "    inputs = tf.keras.Input([5,260,260,3])\n",
    "    x = layers.TimeDistributed(cnn)(inputs)\n",
    "\n",
    "    drop1 = hp.Float(\"drop1\",min_value = 0,max_value = 0.4)\n",
    "\n",
    "    x = layers.TimeDistributed( layers.Dropout(drop1), name = \"drop1\") (x)\n",
    "\n",
    "    hidden_units =  hp.Int(\"hidden_units\",min_value=64, max_value=1028, step=64)\n",
    "\n",
    "    if hp.Choice(\"rnn\",[\"lstm\",\"gru\"]) == \"lstm\":\n",
    "        x = layers.LSTM(hidden_units, name = \"lstm\")(x)\n",
    "    else:\n",
    "        x = layers.GRU(hidden_units, name = \"gru\") (x)\n",
    "\n",
    "    drop2 = hp.Float(\"drop2\",min_value = 0,max_value = 0.4)\n",
    "    x = layers.Dropout(drop2,name =\"drop2\")(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"pred\",kernel_regularizer=tf.keras.regularizers.L2())(x)\n",
    "\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    model = tf.keras.Model(inputs,outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "build_model(kt.HyperParameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 18m 58s]\n",
      "val_accuracy: 0.9120535850524902\n",
      "\n",
      "Best val_accuracy So Far: 0.9254464507102966\n",
      "Total elapsed time: 04h 14m 49s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'units does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Felix\\SVI_Model_ToolBox\\view_directions_sequence\\training\\tuna_search.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Get the optimal hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m best_hps\u001b[39m=\u001b[39mtuner\u001b[39m.\u001b[39mget_best_hyperparameters(num_trials\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mThe hyperparameter search is complete. The optimal number of units in the first densely-connected\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mlayer is \u001b[39m\u001b[39m{\u001b[39;00mbest_hps\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39munits\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m and the optimal learning rate for the optimizer\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mis \u001b[39m\u001b[39m{\u001b[39;00mbest_hps\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Felix/SVI_Model_ToolBox/view_directions_sequence/training/tuna_search.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ual\\.conda\\envs\\felix\\lib\\site-packages\\keras_tuner\\engine\\hyperparameters.py:741\u001b[0m, in \u001b[0;36mHyperParameters.get\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is currently inactive.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[0;32m    740\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 741\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'units does not exist.'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!mkdir tuner_run\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='tuner_run',\n",
    "                     project_name='vd_seq')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_ds, epochs=200, validation_data=val_ds, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "with open(\"tuner_run/best_hps.json\",\"w\") as f:\n",
    "    json.dump(best_hp.values,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(best_hp).save_weights(\"tuner_run/hp_op/weights.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('felix')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64aad402826e5067045a3f2545cdf45209700db649eac24e1eb71cad2d7c755c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
